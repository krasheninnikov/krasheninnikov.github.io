I did my PhD in AI safety at Cambridge with David Krueger and Rich Turner, finishing in December 2025. Why AI safety? I think we might be less than a decade away from AI systems capable of doing everything humans can do using computers -- including further AI research. I want to make sure this won't lead humanity to permanently lose control of our future.

I'm into a pretty broad range of research, lately focusing on a mix of interpretability / science of deep learning / control / security. My claim to fame is coining "out-of-context learning" in an early version of [this paper](/implicit-meta-learning/). More recently, we showed that [language models linearly encode at which point of training they learned a specific fact](/training-order/).

Before Cambridge, I earned my MSc in AI from the University of Amsterdam, and worked with <a href="https://humancompatible.ai/about">UC Berkeley's Center for Human-Compatible AI</a> during and after my studies. I also spent a year at Sony AI Zurich researching deep RL and robotics.


<!-- # Publications

1. <b style="color:#8b0000">Steering clear: a systematic study of activation steering in a toy setup.</b>
<b>Dmitrii Krasheninnikov</b>, David Krueger. MINT workshop at NeurIPS 2024. <a href="https://drive.google.com/file/d/19ESKEpQoNj9wMgdGyOyJsEGWMFsNGfY5/view?usp=sharing">Paper</a>.

1. <b style="color:#8b0000">Comparing bottom-up and top-down steering approaches on in-context learning tasks.</b>
Madeline Brumley, Joe Kwon, David Krueger, <b>Dmitrii Krasheninnikov</b>, Usman Anwar. MINT workshop at NeurIPS 2024. <a href="https://arxiv.org/abs/2411.07213">Paper</a>.




1. <b style="color:#8b0000">Open problems and fundamental limitations of reinforcement learning from human feedback.</b> Stephen Casper\*, Xander Davies\*, and 30 coauthors including <b>Dmitrii Krasheninnikov</b>. TMLR. <a href="https://arxiv.org/abs/2307.15217">Paper</a>.


4. <b style="color:#8b0000">Harms from increasingly agentic algorithmic systems.</b> Alan Chan and 21 coauthors including <b>Dmitrii Krasheninnikov</b>. ACM FAccT 2023. <a href="https://arxiv.org/abs/2302.10329">Paper</a>.

5. <b style="color:#8b0000">Assistance with large language models.</b> <b>Dmitrii Krasheninnikov\*</b>, Egor Krasheninnikov\*, David Krueger. InterNLP, Human in the Loop Learning, and ML Safety workshops at NeurIPS 2022. <a href="https://openreview.net/forum?id=OE9V81spp6B">Paper</a>.


6. <b style="color:#8b0000">Benefits of assistance over reward learning.</b> Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, <b>Dmitrii Krasheninnikov</b>, Lawrence Chan, Michael Dennis, Pieter Abbeel, Anca Dragan, Stuart Russell. Best paper award at the Cooperative AI workshop at NeurIPS 2020. <a href="https://openreview.net/forum?id=DFIoGDZejIB">Paper</a>, <a href="https://github.com/HumanCompatibleAI/assistance-games">code</a>.

7. <b style="color:#8b0000">Combining reward information from multiple sources.</b> <b>Dmitrii Krasheninnikov</b>, Rohin Shah, Herke van Hoof. Learning with Rich Experience and Safety & Robustness in Decision Making workshops at NeurIPS 2019. <a href="https://arxiv.org/abs/2103.12142">Paper</a>, <a href="https://drive.google.com/file/d/1oPG1nfjnVge0Pi0JJYi7x78IGQIeeR2s/view?usp=sharing">poster</a>.

8. <b style="color:#8b0000">Preferences implicit in the state of the world.</b> Rohin Shah\*, <b>Dmitrii Krasheninnikov\*</b>, Jordan Alexander, Anca Dragan, Pieter Abbeel. ICLR 2019. <a href="https://openreview.net/forum?id=rkevMnRqYQ">Paper</a>, <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/">blog post</a>, <a href="https://github.com/HumanCompatibleAI/rlsp/blob/master/poster-preferences-implicit-in-the-state-of-the-world.pdf">poster</a>, <a href="https://github.com/HumanCompatibleAI/rlsp">code</a>. -->



# Selected publications 
<!-- (full list <a href="https://scholar.google.com/citations?user=BIQflKQAAAAJ&hl=en">here</a>) -->

See full list on [Google Scholar](https://scholar.google.com/citations?user=BIQflKQAAAAJ&hl=en).

<!-- 1. [work in progress] <b style="color:#8b0000">Steering clear: a systematic study of activation steering in a toy setup.</b> <b>Dmitrii Krasheninnikov</b>, David Krueger. MINT workshop at NeurIPS 2024. <a href="https://drive.google.com/file/d/19ESKEpQoNj9wMgdGyOyJsEGWMFsNGfY5/view?usp=sharing">Paper</a>. -->

1. <b style="color:#8b0000">Fresh in memory: Training-order recency is linearly encoded in language model activations.</b> <b>Dmitrii Krasheninnikov</b>, Richard E. Turner, David Krueger. Best paper runner-up at the MemFM workshop @ ICML 2025. <a href="https://arxiv.org/abs/2509.14223">Paper</a>, [summary](/training-order/).

2. <b style="color:#8b0000">Detecting High-Stakes Interactions with Activation Probes.</b> Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, <b>Dmitrii Krasheninnikov</b>. NeurIPS 2025; also outstanding paper at the Actionable Interpretability workshop @ ICML 2025. <a href="https://arxiv.org/abs/2506.10805">Paper</a>, [blog post](https://www.lesswrong.com/posts/utcZSRv2JfahD8yfz/detecting-high-stakes-interactions-with-activation-probes).

3. <b style="color:#8b0000">Stress-testing capability elicitation with password-locked models.</b> Ryan Greenblatt\*, Fabien Roger\*, <b>Dmitrii Krasheninnikov</b>, David Krueger. NeurIPS 2024. <a href="https://arxiv.org/abs/2405.19550">Paper</a>, [blog post](https://www.alignmentforum.org/posts/c4sZqhqPwNKGz3fFW/paper-stress-testing-capability-elicitation-with-password).


4. <b style="color:#8b0000">Implicit meta-learning may lead language models to trust more reliable sources.</b> <b>Dmitrii Krasheninnikov\*</b>, Egor Krasheninnikov\*, Bruno Mlodozeniec, David Krueger. ICML 2024. <a href="https://arxiv.org/abs/2310.15047">Paper</a>, [summary](/implicit-meta-learning/), <a href="https://github.com/krasheninnikov/internalization/blob/master/internalization-icml-poster.png">poster</a>, <a href="https://github.com/krasheninnikov/internalization">code</a>.

5. <b style="color:#8b0000">Defining and characterizing reward hacking.</b> Joar Skalse\*, Nikolaus Howe, <b>Dmitrii Krasheninnikov</b>, David Krueger\*. NeurIPS 2022. <a href="https://arxiv.org/abs/2209.13085">Paper</a>.

6. <b style="color:#8b0000">Preferences implicit in the state of the world.</b> Rohin Shah\*, <b>Dmitrii Krasheninnikov\*</b>, Jordan Alexander, Anca Dragan, Pieter Abbeel. ICLR 2019. <a href="https://openreview.net/forum?id=rkevMnRqYQ">Paper</a>, <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/">blog post</a>, <a href="https://github.com/HumanCompatibleAI/rlsp/blob/master/poster-preferences-implicit-in-the-state-of-the-world.pdf">poster</a>, <a href="https://github.com/HumanCompatibleAI/rlsp">code</a>.



\* Equal contribution








<!-- 
<b>Assistance with large language models.</b> <span style="color:red">Dmitrii Krasheninnikov\*</span>, Egor Krasheninnikov\*, David Krueger. InterNLP, Human in the Loop Learning, and ML Safety workshops at NeurIPS 2022. <a href="https://openreview.net/forum?id=OE9V81spp6B">Paper</a>.




<b>Benefits of Assistance over Reward Learning.</b> Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, <span style="color:red">Dmitrii Krasheninnikov</span>, Lawrence Chan, Michael Dennis, Pieter Abbeel, Anca Dragan, Stuart Russell. Best paper award at the Cooperative AI workshop at NeurIPS 2020. <a href="https://openreview.net/forum?id=DFIoGDZejIB">Paper</a>, <a href="https://github.com/HumanCompatibleAI/assistance-games">code</a>.


<b>Combining reward information from multiple sources.</b> <span style="color:red">Dmitrii Krasheninnikov</span>, Rohin Shah, Herke van Hoof. Learning with Rich Experience and Safety & Robustness in Decision Making workshops at NeurIPS 2019. <a href="https://arxiv.org/abs/2103.12142">Paper</a>, <a href="https://drive.google.com/file/d/1oPG1nfjnVge0Pi0JJYi7x78IGQIeeR2s/view?usp=sharing">poster</a>.

<!-- <b style="color:#8b0000">Harms from increasingly agentic algorithmic systems.</b> Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, <b>Dmitrii Krasheninnikov</b>, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, Tegan Maharaj. ACM FAccT 2023. <a href="https://arxiv.org/abs/2302.10329">Paper</a>. -->

<!-- <b style="color:#8b0000">Open problems and fundamental limitations of reinforcement learning from human feedback.</b> Stephen Casper\*, Xander Davies\*, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, <b>Dmitrii Krasheninnikov</b>, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell. Transactions on Machine Learning Research (Outstanding paper finalist). <a href="https://arxiv.org/abs/2307.15217">Paper</a>. -->
