---
layout: page
title: About
permalink: /about/
---


I am a PhD student in machine learning at the University of Cambridge, working on AI safety with David Krueger. I think it's plausible that we'll build AI systems capable of doing *everything humans can do in front of computers* in the next decade, and am hoping to ensure that the (inevitably widespread) deployment of such systems won't lead the humanity to permanently lose control over our future.

<!-- and am hoping to ensure that the long-term impact of such systems' inevitably widespread deployment is positive. -->
<!-- I'm interested in designing AI systems that do what we want them to do, and am hoping to ensure that AI's long-term impact on humanity is positive. -->
<!-- I’m hoping to ensure that powerful AI systems of the future do what we want them to do, with the aim of increasing the likelihood of AI having a positive long-term impact on humanity. -->
 
<!-- Previously, I did a masters in AI at the University of Amsterdam, and had the pleasure to work with UC Berkeley’s <a href="https://humancompatible.ai/">Center for Human-Compatible AI</a> during and after the masters. In addition, I worked on deep RL and robotics at Sony AI Zurich for a bit over a year. -->

I earned my master's degree in AI from the University of Amsterdam, and had the opportunity to work with UC Berkeley's <a href="https://humancompatible.ai/">Center for Human-Compatible AI</a> during and after my studies. I also spent a year working on deep RL and robotics at Sony AI Zurich.


<!-- I am a Research Engineer working on deep RL and robotics at Sony AI. I completed my masters in AI at the University of Amsterdam, and had the pleasure to work with UC Berkeley’s Center for Human-Compatible AI during and after the masters. Next, I'll be starting a PhD at the University of Cambridge, where I'll work on technical AGI safety with David Krueger. -->


<!---Last fall I graduated cum laude with a MSc in AI from the University of Amsterdam, which I was able to attend thanks to a grant from the Open Philanthropy Project. During and after the masters I had the pleasure to work with UC Berkeley's Center for Human-Compatible AI; this collaboration resulted in two papers. Next, I am joining Sony AI in Zurich starting September 2020.->


<!---I am a second year MSc artificial intelligence student at the University of Amsterdam. These days I am focused on figuring out how an AI can robustly infer what humans want from what they do. --->

<!--- My CV can be found <a href="">here</a>.

<!--- In my spare time, I enjoy hiking, bouldering, listening to podcasts and meditating.  --->

# Publications
<!-- <a href="https://scholar.google.com/citations?user=BIQflKQAAAAJ&hl=en">Google scholar profile</a>. -->

<!-- <b>Implicit meta-learning may lead language models to trust more reliable sources.</b> <span style="color:red">Dmitrii Krasheninnikov\*</span>, Egor Krasheninnikov\*, Bruno Mlodozeniec, David Krueger. ICML 2024. <a href="https://arxiv.org/abs/2310.15047">Paper</a>, <a href="https://drive.google.com/file/d/1aZMzo8Dzz20FIoxKhgsY62bjSp-LEuH9/view?usp=sharing">poster</a>, <a href="https://github.com/krasheninnikov/internalization">code</a>. -->


<!-- <b>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.</b> Stephen Casper\*, Xander Davies\*, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, <span style="color:red">Dmitrii Krasheninnikov</span>, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell. <a href="https://arxiv.org/abs/2307.15217">Paper</a>.

<b>Harms from Increasingly Agentic Algorithmic Systems.</b> Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, <span style="color:red">Dmitrii Krasheninnikov</span>, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, Tegan Maharaj. ACM FAccT 2023. <a href="https://arxiv.org/abs/2302.10329">Paper</a>.

<b>Assistance with large language models.</b> <span style="color:red">Dmitrii Krasheninnikov\*</span>, Egor Krasheninnikov\*, David Krueger. InterNLP, Human in the Loop Learning, and ML Safety workshops at NeurIPS 2022. <a href="https://openreview.net/forum?id=OE9V81spp6B">Paper</a>.


<b>Defining and characterizing reward hacking.</b> Joar Skalse\*, Nikolaus Howe, <span style="color:red">Dmitrii Krasheninnikov</span>, David Krueger\*. NeurIPS 2022. <a href="https://arxiv.org/abs/2209.13085">Paper</a>.


<b>Benefits of Assistance over Reward Learning.</b> Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, <span style="color:red">Dmitrii Krasheninnikov</span>, Lawrence Chan, Michael Dennis, Pieter Abbeel, Anca Dragan, Stuart Russell. Best paper award at the Cooperative AI workshop at NeurIPS 2020. <a href="https://openreview.net/forum?id=DFIoGDZejIB">Paper</a>, <a href="https://github.com/HumanCompatibleAI/assistance-games">code</a>.


<b>Combining reward information from multiple sources.</b> <span style="color:red">Dmitrii Krasheninnikov</span>, Rohin Shah, Herke van Hoof. Learning with Rich Experience and Safety & Robustness in Decision Making workshops at NeurIPS 2019. <a href="https://arxiv.org/abs/2103.12142">Paper</a>, <a href="https://drive.google.com/file/d/1oPG1nfjnVge0Pi0JJYi7x78IGQIeeR2s/view?usp=sharing">poster</a>.


<b>Preferences implicit in the state of the world.</b> Rohin Shah\*, <span style="color:red">Dmitrii Krasheninnikov\*</span>, Jordan Alexander, Anca Dragan, Pieter Abbeel. International Conference on Learning Representations (ICLR) 2019. <a href="https://openreview.net/forum?id=rkevMnRqYQ">Paper</a>, <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/">blog post</a>, <a href="https://github.com/HumanCompatibleAI/rlsp/blob/master/poster-preferences-implicit-in-the-state-of-the-world.pdf">ICLR 2019 poster</a>, <a href="https://github.com/HumanCompatibleAI/rlsp">code</a>. -->


<!-- <b style="color:#8b0000">Harms from increasingly agentic algorithmic systems.</b> Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, <b>Dmitrii Krasheninnikov</b>, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, Tegan Maharaj. ACM FAccT 2023. <a href="https://arxiv.org/abs/2302.10329">Paper</a>. -->

<!-- <b style="color:#8b0000">Open problems and fundamental limitations of reinforcement learning from human feedback.</b> Stephen Casper\*, Xander Davies\*, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, <b>Dmitrii Krasheninnikov</b>, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell. Transactions on Machine Learning Research. <a href="https://arxiv.org/abs/2307.15217">Paper</a>. -->

1. <b style="color:#8b0000">Stress-testing capability elicitation with password-locked models.</b> Ryan Greenblatt\*, Fabien Roger\*, <b>Dmitrii Krasheninnikov</b>, David Krueger. <a href="https://arxiv.org/abs/2405.19550">Paper</a>.


2. <b style="color:#8b0000">Implicit meta-learning may lead language models to trust more reliable sources.</b> <b>Dmitrii Krasheninnikov\*</b>, Egor Krasheninnikov\*, Bruno Mlodozeniec, David Krueger. ICML 2024. <a href="https://arxiv.org/abs/2310.15047">Paper</a>, <a href="https://github.com/krasheninnikov/internalization/blob/master/internalization-icml-poster.png">poster</a>, <a href="https://github.com/krasheninnikov/internalization">code</a>.

3. <b style="color:#8b0000">Open problems and fundamental limitations of reinforcement learning from human feedback.</b> Stephen Casper\*, Xander Davies\*, and 30 coauthors including <b>Dmitrii Krasheninnikov</b>. TMLR. <a href="https://arxiv.org/abs/2307.15217">Paper</a>.


4. <b style="color:#8b0000">Harms from increasingly agentic algorithmic systems.</b> Alan Chan and 21 coauthors including <b>Dmitrii Krasheninnikov</b>. ACM FAccT 2023. <a href="https://arxiv.org/abs/2302.10329">Paper</a>.

5. <b style="color:#8b0000">Assistance with large language models.</b> <b>Dmitrii Krasheninnikov\*</b>, Egor Krasheninnikov\*, David Krueger. InterNLP, Human in the Loop Learning, and ML Safety workshops at NeurIPS 2022. <a href="https://openreview.net/forum?id=OE9V81spp6B">Paper</a>.

6. <b style="color:#8b0000">Defining and characterizing reward hacking.</b> Joar Skalse\*, Nikolaus Howe, <b>Dmitrii Krasheninnikov</b>, David Krueger\*. NeurIPS 2022. <a href="https://arxiv.org/abs/2209.13085">Paper</a>.

7. <b style="color:#8b0000">Benefits of assistance over reward learning.</b> Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, <b>Dmitrii Krasheninnikov</b>, Lawrence Chan, Michael Dennis, Pieter Abbeel, Anca Dragan, Stuart Russell. Best paper award at the Cooperative AI workshop at NeurIPS 2020. <a href="https://openreview.net/forum?id=DFIoGDZejIB">Paper</a>, <a href="https://github.com/HumanCompatibleAI/assistance-games">code</a>.

8. <b style="color:#8b0000">Combining reward information from multiple sources.</b> <b>Dmitrii Krasheninnikov</b>, Rohin Shah, Herke van Hoof. Learning with Rich Experience and Safety & Robustness in Decision Making workshops at NeurIPS 2019. <a href="https://arxiv.org/abs/2103.12142">Paper</a>, <a href="https://drive.google.com/file/d/1oPG1nfjnVge0Pi0JJYi7x78IGQIeeR2s/view?usp=sharing">poster</a>.

9. <b style="color:#8b0000">Preferences implicit in the state of the world.</b> Rohin Shah\*, <b>Dmitrii Krasheninnikov\*</b>, Jordan Alexander, Anca Dragan, Pieter Abbeel. ICLR 2019. <a href="https://openreview.net/forum?id=rkevMnRqYQ">Paper</a>, <a href="https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/">blog post</a>, <a href="https://github.com/HumanCompatibleAI/rlsp/blob/master/poster-preferences-implicit-in-the-state-of-the-world.pdf">poster</a>, <a href="https://github.com/HumanCompatibleAI/rlsp">code</a>.


\* Equal contribution




